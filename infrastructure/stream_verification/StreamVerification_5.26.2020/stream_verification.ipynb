{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, fileinput, random, datetime\n",
    "import re, json, csv\n",
    "import itertools, collections\n",
    "import numpy, pandas, math, scipy, scipy.stats\n",
    "import plotly\n",
    "import climata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import climata.usgs as usgs\n",
    "\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "from math import sqrt\n",
    "from IPython.core.display import display, HTML\n",
    "from numpy import nan\n",
    "from pprint import pprint, pformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumb setup stuff\\n\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "# matplotlib.rcParams['figure.figsize'] = [8, 5\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.float_format = '{:,f}'.format\n",
    "\n",
    "my_template = dict(\n",
    "  \n",
    ")\n",
    "\n",
    "pio.templates[\"ians_template\"] = go.layout.Template(\n",
    "  layout=go.Layout(\n",
    "    height=700\n",
    "  )\n",
    ")\n",
    "pio.templates.default = \"ians_template\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dictionary_lists( *dictionaries ):\n",
    "  # Simply use keys from first dictionary\n",
    "  # Ensure later that all dictionaries have these keys\n",
    "  keys = set( dictionaries[0].keys() )\n",
    "  \n",
    "  if len(dictionaries) == 1: \n",
    "    # Check keys are the same by comparing to first\n",
    "    for d in dictionaries[1:]:\n",
    "      if set(d.keys()) != keys:\n",
    "        raise ValueError( \"All dictionaries in merge_dictionary_lists must have same keys\" )\n",
    "    \n",
    "  result_dict = { \n",
    "    key : list( itertools.chain( *( list(d[key]) for d in dictionaries ) ) )\n",
    "    for key in keys \n",
    "  }\n",
    "  \n",
    "  return result_dict\n",
    "  \n",
    "\n",
    "def station_id_cast( station_id ):\n",
    "  if isinstance( station_id, int ):\n",
    "    return f\"{station_id:0>8}\"\n",
    "  if isinstance( station_id, str ):\n",
    "    return station_id_cast( int(station_id, 10) )\n",
    "  if isinstance( station_id, float ):\n",
    "    # check is int-able\n",
    "    if int(math.ceil(station_id)) != int(math.floor(station_id)):\n",
    "      raise ValueError( f\"Cannot create station id from non-integer float {station_id}\" )\n",
    "    return station_id_cast( int(station_id) )\n",
    "  \n",
    "  raise ValueError( f\"station_id_cast given station id as {type(station_id)}, can only handle int, str, and float\" )\n",
    "  \n",
    "\n",
    "def parameter_id_cast( parameter_id ):\n",
    "  if isinstance( parameter_id, int ):\n",
    "    return f\"{parameter_id:0>5}\"\n",
    "  if isinstance( parameter_id, str ):\n",
    "    return parameter_id_cast( int(parameter_id, 10) )\n",
    "  if isinstance( parameter_id, float ):\n",
    "    # check is int-able\n",
    "    if int(math.ceil(parameter_id)) != int(math.floor(parameter_id)):\n",
    "      raise ValueError( f\"Cannot create parameter id from non-integer float {parameter_id}\" )\n",
    "    return parameter_id_cast( int(parameter_id) )\n",
    "  \n",
    "  raise ValueError( f\"parameter_id_cast given parameter id as {type(parameter_id)}, can only handle ints, strings, and floats\" )\n",
    "\n",
    "  \n",
    "def load_station_data_over_date_range_as_DataFrame( station_id, parameter_id, start_date=None, end_date=None, periods=None, date_range=None ):\n",
    "  # Ensure correct invocation of function\n",
    "  if date_range is None :\n",
    "    # for pd.date_range, must define exactly 3 of the 4 time-period parameters (in our case, at least 2 of 3)\n",
    "    # Done by counting how many of the parameters are None\n",
    "    if len( [ i for i in [start_date, end_date, periods] if i is not None] ) < 2:\n",
    "      raise ValueError( \"With date_range unset, at least 2 of the following parameters must be specified: start_date, end_date, and periods\" )\n",
    "  \n",
    "  # Properly cast station and parameter ids\n",
    "  station_id = station_id_cast( station_id )\n",
    "  parameter_id = parameter_id_cast( parameter_id )\n",
    "                       \n",
    "  # Create date-range \n",
    "  # From provided time-period parameters\n",
    "  if date_range is None:\n",
    "    date_range_list = pd.date_range( start=start_date, end=end_date, periods=periods ).tolist()\n",
    "  # From provided list of dates\n",
    "  elif isinstance( date_range, list ):\n",
    "    date_range_list = date_range\n",
    "  # From provided pandas DatetimeIndex\n",
    "  elif isinstance( date_range, pandas.core.indexes.datetimes.DatetimeIndex ):\n",
    "    date_range_list = date_range.tolist()\n",
    "\n",
    "  # Download station data\n",
    "  station_requests = usgs.DailyValueIO(\n",
    "      start_date = date_range_list[0],\n",
    "      end_date   = date_range_list[-1],\n",
    "      station    = station_id,\n",
    "      parameter  = parameter_id\n",
    "    )\n",
    "  # climata.usgs.DailyValueIO claims to be iterable, but in practice there is nothing to iterate over...\n",
    "  # Note the IO obeject annot be exhausted\n",
    "  #(ie, it can be iterated over multiple times to get the different aspects of the request(s) from it)\n",
    "  \n",
    "  # Flatten data into list of dictionaries with the data and flow extracted\n",
    "  # Note this is a generator, which *CAN* be exhausted. Only use *ONCE*!\n",
    "  flattened_data_generator = (\n",
    "    dict(\n",
    "      date=date_value_object.date,\n",
    "      day=date_value_object.date.timetuple()[7]-1,       # Days since January 1st\n",
    "      week=int(math.floor((date_value_object.date.timetuple()[7]-1) / 7)), # Week of the year\n",
    "      month=date_value_object.date.month,\n",
    "      year=date_value_object.date.year,\n",
    "      flow=date_value_object.value\n",
    "    )\n",
    "    for request in station_requests\n",
    "    for date_value_object in request.data\n",
    "  )\n",
    "  \n",
    "  # Create DatFrame from the flattened generator.\n",
    "  station_data = pd.DataFrame( \n",
    "    columns = [\"date\",\"day\",\"week\",\"month\",\"year\",\"flow\"],\n",
    "    data    = flattened_data_generator \n",
    "  ).sort_values(\"date\")\n",
    "  \n",
    "  return station_data\n",
    "\n",
    "\n",
    "def load_station_mapping_as_DataFrame( path_to_stations_table ):\n",
    "  return pd.read_csv( path_to_stations_table )\n",
    "\n",
    "\n",
    "def evaluate_parflow( station_id, parameter_id, stations_map, parflow_data, start_date=None, end_date=None, periods=None, date_range=None, group_by=None ):\n",
    "  station_id = station_id_cast( station_id )\n",
    "  \n",
    "  # Load Station Data\n",
    "  station_data = load_station_data_over_date_range_as_DataFrame( station_id=station_id, parameter_id=parameter_id, start_date=start_date, end_date=end_date, periods=periods, date_range=date_range )\n",
    "  \n",
    "  # Get this station's ParFlow x and y mapping\n",
    "  station_x, station_y = stations_map[ stations_map[\"STNID\"] == int(station_id) ][[\"x_new\",\"y_new\"]].values[0]\n",
    "  \n",
    "  # Filter ParFlow data down to this station.\n",
    "  parflow_station_data = parflow_data[ (parflow_data[\"x\"] == station_x) & (parflow_data[\"y\"] == station_y) ].sort_values(\"date\")\n",
    "  \n",
    "  # Group (as necessary)\n",
    "  if group_by != None:\n",
    "    valid_group_by_keys = [\"week\", \"month\", \"year\"]\n",
    "    if group_by not in valid_group_by_keys:\n",
    "      raise RuntimeError( f\"Cannot group by \\\"{group_by}\\\". Can only group by {', '.join( valid_group_by_keys )}\" )\n",
    "    \n",
    "    grouped_station_data = station_data.groupby( group_by, as_index=False ).aggregate( {\"date\" : \"first\", \"flow\" : \"mean\"} )\n",
    "    grouped_parflow_station_data = parflow_station_data.groupby( group_by, as_index=False ).aggregate( {\"date\" : \"first\", \"flow\" : \"mean\"} )\n",
    "    yaxis_title_modifier = f\"Average {group_by.title()}ly\"                                                                                        \n",
    "  else:\n",
    "    grouped_station_data = station_data\n",
    "    grouped_parflow_station_data = parflow_station_data\n",
    "    yaxis_title_modifier = \"Daily\"\n",
    "    \n",
    "#   print( grouped_station_data )\n",
    "#   print( grouped_parflow_station_data )\n",
    "\n",
    "  # Compute differences\n",
    "  parflow_station_difference = pd.DataFrame( \n",
    "    dict(\n",
    "      date = grouped_station_data[\"date\"],\n",
    "      flow = grouped_parflow_station_data[\"flow\"] - grouped_station_data[\"flow\"]\n",
    "    )\n",
    "  ).sort_values(\"date\")\n",
    "  \n",
    "  parflow_station_difference_squared = pd.DataFrame( \n",
    "    dict(\n",
    "      date = grouped_station_data[\"date\"],\n",
    "      flow = parflow_station_difference[\"flow\"].pow(2)\n",
    "    )\n",
    "  ).sort_values(\"date\")\n",
    "    \n",
    "  \n",
    "  N = grouped_station_data.index.size\n",
    "  parflow_station_difference_sum = parflow_station_difference[\"flow\"].sum()\n",
    "  parflow_station_difference_squared_sum = parflow_station_difference_squared[\"flow\"].sum()\n",
    "\n",
    "  percent_bias = ( parflow_station_difference_sum / grouped_station_data[\"flow\"].sum() )*100\n",
    "  # For scipy.stats.spearmanr, I am not under the impression that the order matters\n",
    "  spearman_rho, spearman_rho_p = scipy.stats.spearmanr( grouped_parflow_station_data[\"flow\"], grouped_station_data[\"flow\"] )\n",
    "  r_squared = 1 - ( parflow_station_difference_squared_sum / (grouped_station_data[\"flow\"] - grouped_station_data[\"flow\"].mean()).pow(2).sum() )\n",
    "  rmse = np.sqrt( parflow_station_difference_squared_sum / N )\n",
    "  \n",
    "  print( f\"% Bias: {percent_bias}%\" )\n",
    "  print( f\"Spearman Rho: {spearman_rho} (p={spearman_rho_p})\" )\n",
    "  print( f\"R2: {r_squared}\" )\n",
    "  print( f\"RMSE: {rmse}\" )\n",
    "\n",
    "  go.Figure(\n",
    "    data=[\n",
    "      go.Scatter( \n",
    "        x=grouped_station_data[\"date\"],\n",
    "        y=grouped_station_data[\"flow\"],\n",
    "        name=\"Actual\",\n",
    "        mode=\"lines+markers\",\n",
    "        line=dict(\n",
    "          color=\"green\"\n",
    "        )\n",
    "      ),\n",
    "      go.Scatter( \n",
    "        x=grouped_parflow_station_data[\"date\"],\n",
    "        y=grouped_parflow_station_data[\"flow\"],\n",
    "        name=\"Parflow\",\n",
    "        mode=\"lines+markers\",\n",
    "        line=dict(\n",
    "          color=\"orange\"\n",
    "        )\n",
    "      ),\n",
    "      go.Scatter( \n",
    "        x=parflow_station_difference[\"date\"],\n",
    "        y=parflow_station_difference[\"flow\"],\n",
    "        name=\"True Difference (ParFlow - Station)\",\n",
    "        mode=\"lines+markers\",\n",
    "        line=dict(\n",
    "          color=\"red\"\n",
    "        )\n",
    "      )\n",
    "    ],\n",
    "    layout=dict(\n",
    "      title=f\"Comparison of Flow between Station {station_id} and ParFlow @ ({station_x}, {station_y})\",\n",
    "      xaxis=dict(\n",
    "        title=\"Date\"\n",
    "      ),\n",
    "      yaxis=dict(\n",
    "        title=f\"{yaxis_title_modifier} Flow\"\n",
    "      )\n",
    "\n",
    "    )\n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stations map\n",
    "stations_map_path = \"NWM_Gage_Adjustments_attribute_table_20200510.csv\"\n",
    "station_map = load_station_mapping_as_DataFrame( stations_map_path )\n",
    "\n",
    "# Station info\n",
    "station_id = station_id_cast( \"01011000\" )\n",
    "parameter_id = parameter_id_cast( \"00060\" )\n",
    "# data-range\n",
    "end_date=\"2010-12-31\"\n",
    "periods=365\n",
    "date_range=pd.date_range(end=end_date, periods=periods).tolist()\n",
    "\n",
    "# Fake some parflow data starting from the actual station date\n",
    "# I assume the data will look something like a table of rows in the form:\n",
    "# x, y, Date, Parameter Value\n",
    "# Here, use few stations the station (x,y,z) associated with station id.\n",
    "parflow_data = pd.DataFrame(\n",
    "  columns = [\"x\", \"y\", \"date\",\"day\",\"week\",\"month\",\"year\", \"flow\"],\n",
    "  data    = merge_dictionary_lists(\n",
    "    *(\n",
    "      {\n",
    "        \"x\" : [ station_map[ station_map[\"STNID\"] == station_id_int ][\"x_new\"].values[0] ] * len(actual_data),\n",
    "        \"y\" : [ station_map[ station_map[\"STNID\"] == station_id_int ][\"y_new\"].values[0] ] * len(actual_data),\n",
    "        \"date\" : actual_data[\"date\"],\n",
    "        \"day\" : actual_data[\"day\"],\n",
    "        \"week\" : actual_data[\"week\"],\n",
    "        \"month\" : actual_data[\"month\"],\n",
    "        \"year\" : actual_data[\"year\"],\n",
    "        \"flow\" : actual_data[\"flow\"] * (random.random() + random.random())  + (np.random.randn(len(date_range)) * np.random.randn(len(date_range)))\n",
    "      }\n",
    "      for station_id_int in [ int( station_id ), 1034000, 1068910 ]\n",
    "      # Hacky let expression\n",
    "      for actual_data in [load_station_data_over_date_range_as_DataFrame( station_id_cast(station_id_int), parameter_id, date_range=date_range)]\n",
    "    )\n",
    "  )\n",
    ")\n",
    "# parflow_data[0:366]\n",
    "# print( load_station_data_over_date_range_as_DataFrame( station_id, parameter_id, date_range=date_range ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_parflow( \n",
    "  station_id,\n",
    "  parameter_id,\n",
    "  station_map,\n",
    "  parflow_data,\n",
    "  date_range=date_range,\n",
    ")\n",
    "evaluate_parflow( \n",
    "  station_id,\n",
    "  parameter_id,\n",
    "  station_map,\n",
    "  parflow_data,\n",
    "  date_range=date_range,\n",
    "  group_by=\"week\"\n",
    ")\n",
    "evaluate_parflow( \n",
    "  station_id,\n",
    "  parameter_id,\n",
    "  station_map,\n",
    "  parflow_data,\n",
    "  date_range=date_range,\n",
    "  group_by=\"month\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
